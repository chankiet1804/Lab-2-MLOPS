{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6723483,"sourceType":"datasetVersion","datasetId":3873453}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Chuẩn bị dữ liệu ban đầu và kiểm tra số lớp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport re # Thêm thư viện re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom gensim.models import Word2Vec, FastText\nimport nltk\n\n# Tải tài nguyên NLTK một cách an toàn\n# Lưu ý: Trên Kaggle, nếu kernel không có internet, việc download sẽ thất bại.\n# Hãy đảm bảo internet được bật cho notebook (Settings -> Internet: ON)\nprint(\"Đang kiểm tra và tải tài nguyên NLTK...\")\ntry:\n    nltk.data.find('corpora/wordnet.zip')\n    print(\"WordNet đã có.\")\nexcept: # Bắt Exception chung vì nltk.downloader.DownloadError có thể không tồn tại\n    print(\"Đang tải WordNet...\")\n    nltk.download('wordnet', quiet=True)\n    print(\"Đã tải WordNet.\")\n\ntry:\n    nltk.data.find('tokenizers/punkt')\n    print(\"Punkt tokenizer đã có.\")\nexcept:\n    print(\"Đang tải Punkt tokenizer...\")\n    nltk.download('punkt', quiet=True)\n    print(\"Đã tải Punkt tokenizer.\")\n\ntry:\n    nltk.data.find('corpora/stopwords')\n    print(\"Stopwords đã có.\")\nexcept:\n    print(\"Đang tải Stopwords...\")\n    nltk.download('stopwords', quiet=True)\n    print(\"Đã tải Stopwords.\")\nprint(\"Hoàn tất kiểm tra tài nguyên NLTK.\")\n\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords as nltk_stopwords\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 200)\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept OSError:\n    print(\"Cảnh báo: Không tìm thấy style 'seaborn-v0_8-whitegrid'. Sử dụng style mặc định.\")\n    pass\n\n# --- TẢI DỮ LIỆU THỰC ---\nDATA_PATH = '/kaggle/input/legal-text-classification-dataset/legal_text_classification.csv' # Đường dẫn như trong file gốc\n# Trên Kaggle, nếu bạn đã upload dataset và nó được mount vào /kaggle/input/your-dataset-name/\n# thì đường dẫn có thể là: DATA_PATH = '/kaggle/input/your-dataset-name/legal_text_classification.csv'\n# Hoặc nếu 'data' là một thư mục trong dataset của bạn:\n# DATA_PATH = '/kaggle/input/your-dataset-name/data/legal_text_classification.csv'\n# Hãy điều chỉnh DATA_PATH nếu cần thiết cho môi trường Kaggle của bạn.\n\ntry:\n    df = pd.read_csv(DATA_PATH, on_bad_lines=\"skip\", engine='python')\n    print(f\"Tải dữ liệu thành công từ {DATA_PATH}. Số dòng: {len(df)}\")\nexcept FileNotFoundError:\n    print(f\"LỖI: Không tìm thấy file dữ liệu tại '{DATA_PATH}'. Vui lòng kiểm tra lại đường dẫn.\")\n    print(\"Chương trình có thể không hoạt động đúng nếu không có dữ liệu.\")\n    # Tạo DataFrame rỗng để tránh lỗi ở các cell sau nếu muốn tiếp tục debug cấu trúc code\n    df = pd.DataFrame(columns=['case_id', 'case_outcome', 'case_title', 'case_text'])\n# --- KẾT THÚC TẢI DỮ LIỆU ---\n\n# Xử lý giá trị thiếu (như trong notebook gốc)\ndf = df.fillna('')\n\n# Tạo cột 'case_text_sum'\nif 'case_title' in df.columns and 'case_text' in df.columns:\n    df['case_text_sum'] = df['case_title'] + \" \" + df['case_text']\n    print(\"Đã tạo cột 'case_text_sum'.\")\nelse:\n    print(\"Cảnh báo: Thiếu cột 'case_title' hoặc 'case_text'. Không thể tạo 'case_text_sum'.\")\n    if 'case_text_sum' not in df.columns: # Nếu chưa có, tạo cột rỗng để tránh lỗi sau này\n        df['case_text_sum'] = \"\"\n\n\n# Mã hóa nhãn\nif 'case_outcome' in df.columns and not df['case_outcome'].empty:\n    le = LabelEncoder()\n    df['case_outcome_num'] = le.fit_transform(df['case_outcome'])\n    num_classes = df['case_outcome_num'].nunique()\n    print(f\"Đã mã hóa 'case_outcome'. Số lượng lớp (num_classes): {num_classes}\")\n    print(\"Mapping nhãn -> số:\")\n    for i, class_name in enumerate(le.classes_):\n        print(f\"  {class_name} -> {i}\")\nelse:\n    print(\"Cảnh báo: Không tìm thấy cột 'case_outcome' hoặc cột rỗng. Đặt num_classes mặc định là 10.\")\n    num_classes = 10\n    # Khởi tạo le rỗng để tránh lỗi nếu các cell sau cố gắng dùng le.classes_\n    le = LabelEncoder()\n\n\n# --- TIỀN XỬ LÝ VĂN BẢN ---\n# Hàm làm sạch (tương tự notebook gốc, có chỉnh sửa một chút cho rõ ràng)\ndef clean_text_revised(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    # Loại bỏ URL trước, rồi mới loại bỏ ký tự đặc biệt\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Regex chính xác hơn cho URL\n    \n    # Loại bỏ các ký tự không phải chữ cái, số và khoảng trắng (giữ lại chữ và số)\n    # text = re.sub(r'[^a-z0-9\\s]', '', text) # Cách này có thể giữ lại số nếu muốn\n    # Theo notebook gốc, loại bỏ cả số và nhiều ký tự đặc biệt:\n    marks_and_digits = r'''!()-[]{};?@#$%:'\"\\\\,|./^&;*_0123456789'''\n    text = ''.join(char for char in text if char not in marks_and_digits)\n\n    # Loại bỏ các cụm từ không mong muốn\n    unwanted_phrases = ['url', 'privacy policy', 'disclaimers', 'disclaimer', 'copyright policy']\n    for phrase in unwanted_phrases:\n        text = text.replace(phrase, '')\n        \n    text = re.sub(r'\\s+', ' ', text).strip() # Loại bỏ khoảng trắng thừa\n    return text\n\nprint(\"Bắt đầu làm sạch cột 'case_text_sum'...\")\ndf['clean_text'] = df['case_text_sum'].apply(clean_text_revised)\nprint(\"Hoàn thành làm sạch văn bản.\")\n\n\n# Hàm tokenize và áp dụng stemming/lemmatization\nstop_words_list = nltk_stopwords.words('english')\nporter_stemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef tokenize_and_process(text, stop_words, processor_func=None, processor_type=None):\n    if not isinstance(text, str):\n        return []\n    tokens = text.split() # Tokenizer đơn giản bằng split\n    # tokens = nltk.word_tokenize(text) # Lựa chọn tokenizer tốt hơn\n    \n    processed_tokens = []\n    for word in tokens:\n        if word not in stop_words and len(word) > 2: # Lọc stop words và từ ngắn\n            if processor_type == 'stem' and processor_func:\n                processed_tokens.append(processor_func(word))\n            elif processor_type == 'lem' and processor_func:\n                processed_tokens.append(processor_func(word, pos='v')) # Lemmatize động từ\n            elif not processor_type: # Chỉ tokenize và loại bỏ stop words\n                 processed_tokens.append(word)\n            # else: processed_tokens.append(word) # Trường hợp không stem/lem nhưng vẫn giữ từ\n    return processed_tokens\n\nprint(\"Bắt đầu tokenizing, stemming và lemmatization...\")\ndf['tokens_stm'] = df['clean_text'].apply(lambda x: tokenize_and_process(x, stop_words_list, porter_stemmer.stem, 'stem'))\ndf['tokens_lem'] = df['clean_text'].apply(lambda x: tokenize_and_process(x, stop_words_list, wordnet_lemmatizer.lemmatize, 'lem'))\n\n# Tạo lại chuỗi văn bản đã xử lý\ndf['text_stm_joined'] = df['tokens_stm'].apply(' '.join)\ndf['text_lem_joined'] = df['tokens_lem'].apply(' '.join)\nprint(\"Hoàn thành tokenizing, stemming, lemmatization và tạo chuỗi văn bản.\")\nprint(\"\\nDataFrame head sau tiền xử lý cơ bản:\")\nprint(df[['case_outcome', 'case_outcome_num', 'clean_text', 'tokens_stm', 'tokens_lem']].head(2))\n# --- KẾT THÚC PHẦN TIỀN XỬ LÝ ---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T18:00:04.567471Z","iopub.execute_input":"2025-05-20T18:00:04.567685Z","iopub.status.idle":"2025-05-20T18:00:10.029995Z","shell.execute_reply.started":"2025-05-20T18:00:04.567667Z","shell.execute_reply":"2025-05-20T18:00:10.028855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset_path = '/kaggle/input/legal-text/cleaned_legal_text.csv'\n# df = pd.read_csv(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.491953Z","iopub.status.idle":"2025-05-20T17:45:12.492302Z","shell.execute_reply.started":"2025-05-20T17:45:12.492111Z","shell.execute_reply":"2025-05-20T17:45:12.492127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# le = LabelEncoder()\n# df['case_outcome_num'] = le.fit_transform(df['case_outcome'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.493655Z","iopub.status.idle":"2025-05-20T17:45:12.493989Z","shell.execute_reply.started":"2025-05-20T17:45:12.493821Z","shell.execute_reply":"2025-05-20T17:45:12.493835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dữ liệu cho stemming\nX_stm_tokens = df['tokens_stm'] # Series of lists of tokens\nX_stm_text = df['text_stm_joined'] # Series of strings\ny_stm = df['case_outcome_num']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.495860Z","iopub.status.idle":"2025-05-20T17:45:12.496108Z","shell.execute_reply.started":"2025-05-20T17:45:12.495976Z","shell.execute_reply":"2025-05-20T17:45:12.495985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dữ liệu cho lemmatization\nX_lem_tokens = df['tokens_lem'] # Series of lists of tokens\nX_lem_text = df['text_lem_joined'] # Series of strings\ny_lem = df['case_outcome_num']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.497151Z","iopub.status.idle":"2025-05-20T17:45:12.497463Z","shell.execute_reply.started":"2025-05-20T17:45:12.497313Z","shell.execute_reply":"2025-05-20T17:45:12.497326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split cho stemming data\ntrain_stm_tokens, test_stm_tokens, train_stm_text, test_stm_text, y_train_stm, y_test_stm = train_test_split(\n    X_stm_tokens, X_stm_text, y_stm, test_size=0.2, random_state=42, stratify=y_stm\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.498077Z","iopub.status.idle":"2025-05-20T17:45:12.498391Z","shell.execute_reply.started":"2025-05-20T17:45:12.498238Z","shell.execute_reply":"2025-05-20T17:45:12.498252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split cho lemmatization data\ntrain_lem_tokens, test_lem_tokens, train_lem_text, test_lem_text, y_train_lem, y_test_lem = train_test_split(\n    X_lem_tokens, X_lem_text, y_lem, test_size=0.2, random_state=42, stratify=y_lem\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.499358Z","iopub.status.idle":"2025-05-20T17:45:12.499659Z","shell.execute_reply.started":"2025-05-20T17:45:12.499508Z","shell.execute_reply":"2025-05-20T17:45:12.499522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Kích thước tập huấn luyện (stemming tokens):\", len(train_stm_tokens))\nprint(\"Kích thước tập kiểm tra (stemming tokens):\", len(test_stm_tokens))\nprint(\"Kích thước tập huấn luyện (lemmatization text):\", len(train_lem_text))\nprint(\"Kích thước tập kiểm tra (lemmatization text):\", len(test_lem_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.500659Z","iopub.status.idle":"2025-05-20T17:45:12.500963Z","shell.execute_reply.started":"2025-05-20T17:45:12.500811Z","shell.execute_reply":"2025-05-20T17:45:12.500825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib # Để lưu model scikit-learn\nimport os\n\n# Tạo thư mục để lưu models nếu chưa có\nMODEL_SAVE_DIR = \"saved_models\"\nif not os.path.exists(MODEL_SAVE_DIR):\n    os.makedirs(MODEL_SAVE_DIR)\n\nbest_f1_score_global = -1.0 # Theo dõi F1 score tốt nhất qua tất cả các mô hình\nbest_model_path_global = \"\"\nbest_model_type_global = \"\" # 'sklearn' hoặc 'pytorch'\n\ndef compute_metrics_updated(y_true, y_pred, model_name=\"\"):\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    print(f\"Kết quả cho {model_name}:\")\n    print(f\"  Accuracy: {accuracy*100:.3f}%\")\n    print(f\"  F1 Score (Macro): {f1*100:.3f}%\")\n    \n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(max(6, num_classes // 2), max(4, num_classes // 2.5))) # Điều chỉnh kích thước dựa trên số lớp\n    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', \n                xticklabels=le.classes_ if 'le' in globals() and hasattr(le, 'classes_') else range(num_classes), \n                yticklabels=le.classes_ if 'le' in globals() and hasattr(le, 'classes_') else range(num_classes))\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    plt.show()\n    return accuracy, f1\n\ndef train_and_evaluate_ml_model(model, X_train, y_train, X_test, y_test, model_name=\"\"):\n    global best_f1_score_global, best_model_path_global, best_model_type_global # Khai báo sử dụng biến global\n\n    print(f\"\\n--- Huấn luyện và Đánh giá: {model_name} ---\")\n    model.fit(X_train, y_train)\n    \n    print(\"\\nTrên tập huấn luyện:\")\n    y_train_pred = model.predict(X_train)\n    compute_metrics_updated(y_train, y_train_pred, f\"{model_name} (Train)\")\n    \n    print(\"\\nTrên tập kiểm tra:\")\n    y_test_pred = model.predict(X_test)\n    acc_test, f1_test = compute_metrics_updated(y_test, y_test_pred, f\"{model_name} (Test)\")\n\n    # Lưu model nếu nó tốt nhất\n    if f1_test > best_f1_score_global:\n        best_f1_score_global = f1_test\n        best_model_path_global = os.path.join(MODEL_SAVE_DIR, f\"best_sklearn_model_{model_name.replace(' ', '_')}.joblib\")\n        best_model_type_global = 'sklearn'\n        try:\n            joblib.dump(model, best_model_path_global)\n            print(f\"Đã lưu model ML tốt nhất mới: {model_name} với F1 (Test): {f1_test:.4f} tại {best_model_path_global}\")\n        except Exception as e:\n            print(f\"Lỗi khi lưu model {model_name}: {e}\")\n            \n    return acc_test, f1_test, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.502287Z","iopub.status.idle":"2025-05-20T17:45:12.502582Z","shell.execute_reply.started":"2025-05-20T17:45:12.502432Z","shell.execute_reply":"2025-05-20T17:45:12.502445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_FEATURES_ML = 2500\n\n# 1. CountVectorizer\nprint(\"Vectorizing với CountVectorizer...\")\ncount_vec_stm = CountVectorizer(max_features=MAX_FEATURES_ML)\nX_train_stm_count = count_vec_stm.fit_transform(train_stm_text)\nX_test_stm_count = count_vec_stm.transform(test_stm_text)\n\ncount_vec_lem = CountVectorizer(max_features=MAX_FEATURES_ML)\nX_train_lem_count = count_vec_lem.fit_transform(train_lem_text)\nX_test_lem_count = count_vec_lem.transform(test_lem_text)\n\n# 2. TfidfVectorizer\nprint(\"Vectorizing với TfidfVectorizer...\")\ntfidf_vec_stm = TfidfVectorizer(max_features=MAX_FEATURES_ML)\nX_train_stm_tfidf = tfidf_vec_stm.fit_transform(train_stm_text)\nX_test_stm_tfidf = tfidf_vec_stm.transform(test_stm_text)\n\ntfidf_vec_lem = TfidfVectorizer(max_features=MAX_FEATURES_ML)\nX_train_lem_tfidf = tfidf_vec_lem.fit_transform(train_lem_text)\nX_test_lem_tfidf = tfidf_vec_lem.transform(test_lem_text)\n\n# 3. Word2Vec\nprint(\"Huấn luyện Word2Vec và tạo document vectors...\")\nW2V_SIZE = 300\nW2V_WINDOW = 5\nW2V_MIN_COUNT = 3\nW2V_WORKERS = 4\n\ndef create_document_vector(tokens_list_series, model, num_features):\n    document_vectors = np.zeros((len(tokens_list_series), num_features))\n    for i, tokens in enumerate(tokens_list_series): # tokens_list_series là Series of lists\n        feature_vec = np.zeros((num_features,), dtype=\"float32\")\n        nwords = 0\n        for word in tokens: # tokens ở đây là list các từ của 1 document\n            if word in model.wv:\n                nwords = nwords + 1\n                feature_vec = np.add(feature_vec, model.wv[word])\n        if nwords > 0:\n            feature_vec = np.divide(feature_vec, nwords)\n        document_vectors[i] = feature_vec\n    return document_vectors\n\n# Stemming\nw2v_model_stm = Word2Vec(sentences=train_stm_tokens, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=W2V_WORKERS)\nX_train_stm_w2v = create_document_vector(train_stm_tokens, w2v_model_stm, W2V_SIZE)\nX_test_stm_w2v = create_document_vector(test_stm_tokens, w2v_model_stm, W2V_SIZE)\n\n# Lemmatization\nw2v_model_lem = Word2Vec(sentences=train_lem_tokens, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=W2V_WORKERS)\nX_train_lem_w2v = create_document_vector(train_lem_tokens, w2v_model_lem, W2V_SIZE)\nX_test_lem_w2v = create_document_vector(test_lem_tokens, w2v_model_lem, W2V_SIZE)\n\n# 4. FastText\nprint(\"Huấn luyện FastText và tạo document vectors...\")\n# Stemming\nft_model_stm = FastText(sentences=train_stm_tokens, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=W2V_WORKERS, sg=1)\nX_train_stm_ft = create_document_vector(train_stm_tokens, ft_model_stm, W2V_SIZE)\nX_test_stm_ft = create_document_vector(test_stm_tokens, ft_model_stm, W2V_SIZE)\n\n# Lemmatization\nft_model_lem = FastText(sentences=train_lem_tokens, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=W2V_WORKERS, sg=1)\nX_train_lem_ft = create_document_vector(train_lem_tokens, ft_model_lem, W2V_SIZE)\nX_test_lem_ft = create_document_vector(test_lem_tokens, ft_model_lem, W2V_SIZE)\n\nprint(\"Đã hoàn thành Vectorization cho ML.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.503845Z","iopub.status.idle":"2025-05-20T17:45:12.504147Z","shell.execute_reply.started":"2025-05-20T17:45:12.503984Z","shell.execute_reply":"2025-05-20T17:45:12.504000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Huấn luyện các mô hình ML truyền thống\n\nresults_ml = [] \n\nml_configurations = [\n    # CountVectorizer\n    {\"name\": \"LR_CountVec_Stm\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_stm_count, \"X_test\": X_test_stm_count, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LR_CountVec_Lem\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_lem_count, \"X_test\": X_test_lem_count, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"MNB_CountVec_Stm\", \"model\": MultinomialNB(alpha=1.0), \"X_train\": X_train_stm_count, \"X_test\": X_test_stm_count, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"MNB_CountVec_Lem\", \"model\": MultinomialNB(alpha=1.0), \"X_train\": X_train_lem_count, \"X_test\": X_test_lem_count, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    # SỬA DUAL Ở ĐÂY\n    {\"name\": \"LSVC_CountVec_Stm\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_stm_count, \"X_test\": X_test_stm_count, \"y_train\": y_train_stm, \"y_test\": y_test_stm}, \n    {\"name\": \"LSVC_CountVec_Lem\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_lem_count, \"X_test\": X_test_lem_count, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"KNN_CountVec_Stm\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_stm_count, \"X_test\": X_test_stm_count, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"KNN_CountVec_Lem\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_lem_count, \"X_test\": X_test_lem_count, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n\n    # TfidfVectorizer\n    {\"name\": \"LR_Tfidf_Stm\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_stm_tfidf, \"X_test\": X_test_stm_tfidf, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LR_Tfidf_Lem\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_lem_tfidf, \"X_test\": X_test_lem_tfidf, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"MNB_Tfidf_Stm\", \"model\": MultinomialNB(alpha=1.0), \"X_train\": X_train_stm_tfidf, \"X_test\": X_test_stm_tfidf, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"MNB_Tfidf_Lem\", \"model\": MultinomialNB(alpha=1.0), \"X_train\": X_train_lem_tfidf, \"X_test\": X_test_lem_tfidf, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    # SỬA DUAL Ở ĐÂY\n    {\"name\": \"LSVC_Tfidf_Stm\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_stm_tfidf, \"X_test\": X_test_stm_tfidf, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LSVC_Tfidf_Lem\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_lem_tfidf, \"X_test\": X_test_lem_tfidf, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"KNN_Tfidf_Stm\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_stm_tfidf, \"X_test\": X_test_stm_tfidf, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"KNN_Tfidf_Lem\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_lem_tfidf, \"X_test\": X_test_lem_tfidf, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    \n    # Word2Vec (sử dụng ma trận dày)\n    {\"name\": \"LR_W2V_Stm\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_stm_w2v, \"X_test\": X_test_stm_w2v, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LR_W2V_Lem\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_lem_w2v, \"X_test\": X_test_lem_w2v, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    # SỬA DUAL Ở ĐÂY\n    {\"name\": \"LSVC_W2V_Stm\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_stm_w2v, \"X_test\": X_test_stm_w2v, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    # Với Word2Vec, n_features (W2V_SIZE) có thể nhỏ, dual=True có thể phù hợp hơn.\n    # Tuy nhiên, để nhất quán và tránh lỗi, ta thử dual=False trước.\n    {\"name\": \"LSVC_W2V_Lem\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_lem_w2v, \"X_test\": X_test_lem_w2v, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"KNN_W2V_Stm\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_stm_w2v, \"X_test\": X_test_stm_w2v, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"KNN_W2V_Lem\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_lem_w2v, \"X_test\": X_test_lem_w2v, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n\n    # FastText (sử dụng ma trận dày)\n    {\"name\": \"LR_FT_Stm\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_stm_ft, \"X_test\": X_test_stm_ft, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LR_FT_Lem\", \"model\": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0, max_iter=1000), \"X_train\": X_train_lem_ft, \"X_test\": X_test_lem_ft, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    # SỬA DUAL Ở ĐÂY\n    {\"name\": \"LSVC_FT_Stm\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_stm_ft, \"X_test\": X_test_stm_ft, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"LSVC_FT_Lem\", \"model\": LinearSVC(class_weight='balanced', dual=False, C=1.0, max_iter=2000), \"X_train\": X_train_lem_ft, \"X_test\": X_test_lem_ft, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n    {\"name\": \"KNN_FT_Stm\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_stm_ft, \"X_test\": X_test_stm_ft, \"y_train\": y_train_stm, \"y_test\": y_test_stm},\n    {\"name\": \"KNN_FT_Lem\", \"model\": KNeighborsClassifier(n_neighbors=10), \"X_train\": X_train_lem_ft, \"X_test\": X_test_lem_ft, \"y_train\": y_train_lem, \"y_test\": y_test_lem},\n]\n\nfor config in ml_configurations:\n    X_train_to_use = config[\"X_train\"]\n    X_test_to_use = config[\"X_test\"]\n    \n    if \"MNB\" in config[\"name\"] and (\"W2V\" in config[\"name\"] or \"FT\" in config[\"name\"]):\n        print(f\"Bỏ qua {config['name']} vì MNB không phù hợp với dense embeddings W2V/FT.\")\n        continue\n        \n    acc, f1, _ = train_and_evaluate_ml_model(\n        config[\"model\"],\n        X_train_to_use,\n        config[\"y_train\"],\n        X_test_to_use,\n        config[\"y_test\"],\n        config[\"name\"]\n    )\n    results_ml.append({\"Model\": config[\"name\"], \"Accuracy_Test\": acc, \"F1_Macro_Test\": f1})\n\nresults_ml_df = pd.DataFrame(results_ml)\nprint(\"\\n--- Bảng tổng kết kết quả ML truyền thống ---\")\nif not results_ml_df.empty:\n    print(results_ml_df.sort_values(by=\"F1_Macro_Test\", ascending=False))\nelse:\n    print(\"Không có kết quả ML nào để hiển thị (results_ml_df rỗng).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.504934Z","iopub.status.idle":"2025-05-20T17:45:12.505247Z","shell.execute_reply.started":"2025-05-20T17:45:12.505062Z","shell.execute_reply":"2025-05-20T17:45:12.505077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Phần Mô hình CNN ---\nVOCAB_SIZE_CNN_BASE = 5000 # Kích thước vocab cơ sở cho CNN, sẽ được cập nhật theo vectorizer\nEMBEDDING_DIM_CNN = 128\nHIDDEN_SIZE_CNN = 64\nMAX_SEQ_LENGTH_CNN = 200\nDROPOUT_RATE_CNN = 0.4 # Tăng dropout\nEPOCHS_CNN = 15 # Tăng epochs một chút\nBATCH_SIZE_CNN = 64\nLEARNING_RATE_CNN = 0.001\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nSử dụng thiết bị: {device} cho CNN\")\n\nclass CNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, dropout_rate, padding_idx=0):\n        super(CNNClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        \n        # Sử dụng nhiều kernel_size khác nhau\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, hidden_size, kernel_size=ks, padding=(ks-1)//2) for ks in [3, 4, 5]\n        ])\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        # Số lượng filters là hidden_size * số lượng conv layers\n        self.fc = nn.Linear(len(self.convs) * hidden_size, num_classes)\n\n    def forward(self, x): # x: (batch_size, seq_len)\n        embedded = self.embedding(x)  # -> (batch_size, seq_len, embedding_dim)\n        embedded = embedded.permute(0, 2, 1)  # -> (batch_size, embedding_dim, seq_len)\n        \n        # Áp dụng từng lớp conv và max pooling\n        conved_outputs = []\n        for conv in self.convs:\n            conved = self.relu(conv(embedded)) # -> (batch_size, hidden_size, seq_len_after_conv)\n            # Global Max Pooling over time\n            pooled = torch.max(conved, dim=2)[0] # -> (batch_size, hidden_size)\n            conved_outputs.append(pooled)\n            \n        # Nối output từ các filters khác nhau\n        concatenated = torch.cat(conved_outputs, dim=1) # -> (batch_size, len(convs) * hidden_size)\n        \n        dropped = self.dropout(concatenated)\n        output = self.fc(dropped) # -> (batch_size, num_classes)\n        return output\n\ndef text_to_ids_cnn(text_tokens, vocab_map, max_seq_len, unk_id=0, pad_id=0):\n    ids = [vocab_map.get(token, unk_id) for token in text_tokens]\n    if len(ids) > max_seq_len:\n        ids = ids[:max_seq_len]\n    else:\n        ids.extend([pad_id] * (max_seq_len - len(ids)))\n    return ids\n\nclass TextDataset(Dataset):\n    def __init__(self, tokens_series, labels_series, vocab_map, max_seq_len, unk_id=0, pad_id=0):\n        self.labels = labels_series.tolist()\n        self.texts_ids = [text_to_ids_cnn(tokens, vocab_map, max_seq_len, unk_id, pad_id) for tokens in tokens_series]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts_ids[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n\ndef train_cnn_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc, total_count = 0, 0, 0\n    for text_batch, label_batch in dataloader:\n        text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n        optimizer.zero_grad()\n        predictions = model(text_batch)\n        loss = criterion(predictions, label_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += (predictions.argmax(1) == label_batch).sum().item()\n        total_count += label_batch.size(0)\n    return total_loss / len(dataloader), total_acc / total_count\n\ndef evaluate_cnn(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_acc, total_count = 0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for text_batch, label_batch in dataloader:\n            text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n            predictions = model(text_batch)\n            loss = criterion(predictions, label_batch)\n            total_loss += loss.item()\n            total_acc += (predictions.argmax(1) == label_batch).sum().item()\n            total_count += label_batch.size(0)\n            all_preds.extend(predictions.argmax(1).cpu().tolist())\n            all_labels.extend(label_batch.cpu().tolist())\n    return total_loss / len(dataloader), total_acc / total_count, all_labels, all_preds\n\nresults_cnn = [] # List để lưu kết quả CNN# --- Phần Mô hình CNN ---\nVOCAB_SIZE_CNN_BASE = 5000 # Kích thước vocab cơ sở cho CNN, sẽ được cập nhật theo vectorizer\nEMBEDDING_DIM_CNN = 128\nHIDDEN_SIZE_CNN = 64\nMAX_SEQ_LENGTH_CNN = 200\nDROPOUT_RATE_CNN = 0.4 # Tăng dropout\nEPOCHS_CNN = 15 # Tăng epochs một chút\nBATCH_SIZE_CNN = 64\nLEARNING_RATE_CNN = 0.001\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nSử dụng thiết bị: {device} cho CNN\")\n\nclass CNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, dropout_rate, padding_idx=0):\n        super(CNNClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        \n        # Sử dụng nhiều kernel_size khác nhau\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, hidden_size, kernel_size=ks, padding=(ks-1)//2) for ks in [3, 4, 5]\n        ])\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        # Số lượng filters là hidden_size * số lượng conv layers\n        self.fc = nn.Linear(len(self.convs) * hidden_size, num_classes)\n\n    def forward(self, x): # x: (batch_size, seq_len)\n        embedded = self.embedding(x)  # -> (batch_size, seq_len, embedding_dim)\n        embedded = embedded.permute(0, 2, 1)  # -> (batch_size, embedding_dim, seq_len)\n        \n        # Áp dụng từng lớp conv và max pooling\n        conved_outputs = []\n        for conv in self.convs:\n            conved = self.relu(conv(embedded)) # -> (batch_size, hidden_size, seq_len_after_conv)\n            # Global Max Pooling over time\n            pooled = torch.max(conved, dim=2)[0] # -> (batch_size, hidden_size)\n            conved_outputs.append(pooled)\n            \n        # Nối output từ các filters khác nhau\n        concatenated = torch.cat(conved_outputs, dim=1) # -> (batch_size, len(convs) * hidden_size)\n        \n        dropped = self.dropout(concatenated)\n        output = self.fc(dropped) # -> (batch_size, num_classes)\n        return output\n\ndef text_to_ids_cnn(text_tokens, vocab_map, max_seq_len, unk_id=0, pad_id=0):\n    ids = [vocab_map.get(token, unk_id) for token in text_tokens]\n    if len(ids) > max_seq_len:\n        ids = ids[:max_seq_len]\n    else:\n        ids.extend([pad_id] * (max_seq_len - len(ids)))\n    return ids\n\nclass TextDataset(Dataset):\n    def __init__(self, tokens_series, labels_series, vocab_map, max_seq_len, unk_id=0, pad_id=0):\n        self.labels = labels_series.tolist()\n        self.texts_ids = [text_to_ids_cnn(tokens, vocab_map, max_seq_len, unk_id, pad_id) for tokens in tokens_series]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts_ids[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n\ndef train_cnn_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc, total_count = 0, 0, 0\n    for text_batch, label_batch in dataloader:\n        text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n        optimizer.zero_grad()\n        predictions = model(text_batch)\n        loss = criterion(predictions, label_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += (predictions.argmax(1) == label_batch).sum().item()\n        total_count += label_batch.size(0)\n    return total_loss / len(dataloader), total_acc / total_count\n\ndef evaluate_cnn(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_acc, total_count = 0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for text_batch, label_batch in dataloader:\n            text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n            predictions = model(text_batch)\n            loss = criterion(predictions, label_batch)\n            total_loss += loss.item()\n            total_acc += (predictions.argmax(1) == label_batch).sum().item()\n            total_count += label_batch.size(0)\n            all_preds.extend(predictions.argmax(1).cpu().tolist())\n            all_labels.extend(label_batch.cpu().tolist())\n    return total_loss / len(dataloader), total_acc / total_count, all_labels, all_preds\n\nresults_cnn = [] # List để lưu kết quả CNN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.506257Z","iopub.status.idle":"2025-05-20T17:45:12.506535Z","shell.execute_reply.started":"2025-05-20T17:45:12.506369Z","shell.execute_reply":"2025-05-20T17:45:12.506383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_cnn_experiment(\n    model_class, \n    train_tokens_series, test_tokens_series, \n    y_train_series, y_test_series,\n    vocab_source_for_cnn, \n    preprocessing_type, \n    embedding_dim, hidden_size, num_classes_cnn, dropout_rate,\n    max_seq_len, epochs, batch_size, lr,\n    count_vectorizer_instance=None, \n    tfidf_vectorizer_instance=None,\n    w2v_model_instance=None,\n    ft_model_instance=None\n    ):\n    global best_f1_score_global, best_model_path_global, best_model_type_global # Khai báo sử dụng biến global\n    \n    model_full_name = f\"CNN_{vocab_source_for_cnn.upper()}_{preprocessing_type.upper()}\"\n    print(f\"\\n--- Chạy thử nghiệm CNN: {model_full_name} ---\")\n\n    UNK_ID = 0 \n    PAD_ID = 0 \n\n    original_vectors = None # Khởi tạo để tránh lỗi nếu không phải w2v/ft\n\n    if vocab_source_for_cnn == 'countvec':\n        vocab_map = count_vectorizer_instance.vocabulary_\n        actual_vocab_size = max(vocab_map.values()) + 1 if vocab_map else 1\n    elif vocab_source_for_cnn == 'tfidfvec':\n        vocab_map = tfidf_vectorizer_instance.vocabulary_\n        actual_vocab_size = max(vocab_map.values()) + 1 if vocab_map else 1\n    elif vocab_source_for_cnn == 'w2v':\n        vocab_map_orig = w2v_model_instance.wv.key_to_index\n        actual_vocab_size = len(w2v_model_instance.wv)\n        idx = 0; standard_vocab_map = {}; original_vectors = []\n        for word in w2v_model_instance.wv.index_to_key:\n            standard_vocab_map[word] = idx\n            original_vectors.append(w2v_model_instance.wv[word])\n            idx +=1\n        vocab_map = standard_vocab_map\n        actual_vocab_size = len(vocab_map)\n    elif vocab_source_for_cnn == 'ft':\n        vocab_map_orig = ft_model_instance.wv.key_to_index\n        actual_vocab_size = len(ft_model_instance.wv)\n        idx = 0; standard_vocab_map = {}; original_vectors = []\n        for word in ft_model_instance.wv.index_to_key:\n            standard_vocab_map[word] = idx\n            original_vectors.append(ft_model_instance.wv[word])\n            idx +=1\n        vocab_map = standard_vocab_map\n        actual_vocab_size = len(vocab_map)\n    else:\n        raise ValueError(\"vocab_source_for_cnn không hợp lệ.\")\n    \n    print(f\"Kích thước vocab thực tế cho Embedding: {actual_vocab_size}\")\n\n    train_dataset = TextDataset(train_tokens_series, y_train_series, vocab_map, max_seq_len, UNK_ID, PAD_ID)\n    test_dataset = TextDataset(test_tokens_series, y_test_series, vocab_map, max_seq_len, UNK_ID, PAD_ID)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    cnn_model = model_class(actual_vocab_size, embedding_dim, hidden_size, num_classes_cnn, dropout_rate, padding_idx=PAD_ID).to(device)\n    \n    if (vocab_source_for_cnn == 'w2v' or vocab_source_for_cnn == 'ft') and original_vectors is not None:\n        pretrained_weights_np = np.array(original_vectors)\n        # Đảm bảo rằng ma trận trọng số không rỗng và có đúng số chiều embedding\n        if pretrained_weights_np.size > 0 and pretrained_weights_np.shape[1] == cnn_model.embedding.embedding_dim:\n            # Nếu vocab size của model lớn hơn vocab của pretrain (ví dụ do thêm PAD/UNK token vào embedding layer)\n            # Tạo ma trận mới và copy trọng số vào\n            final_weights = torch.FloatTensor(cnn_model.embedding.num_embeddings, cnn_model.embedding.embedding_dim).uniform_(-0.05, 0.05)\n            \n            # Số lượng từ trong vocab đã chuẩn hóa của chúng ta (từ W2V/FT)\n            num_pretrained_words = pretrained_weights_np.shape[0] \n            \n            # Lớp embedding của chúng ta có thể lớn hơn (actual_vocab_size) nếu có token PAD/UNK riêng\n            # Tuy nhiên, với logic hiện tại, actual_vocab_size = num_pretrained_words\n            # Chúng ta sẽ copy trực tiếp nếu kích thước khớp\n            if num_pretrained_words == actual_vocab_size:\n                 final_weights = torch.FloatTensor(pretrained_weights_np)\n            elif num_pretrained_words < actual_vocab_size:\n                # Trường hợp này ít xảy ra với logic hiện tại nhưng để phòng ngừa\n                final_weights[:num_pretrained_words, :] = torch.FloatTensor(pretrained_weights_np)\n                print(f\"Cảnh báo: Vocab của Embedding Layer ({actual_vocab_size}) lớn hơn vocab pretrain ({num_pretrained_words}). Phần còn lại được khởi tạo ngẫu nhiên.\")\n            else: # num_pretrained_words > actual_vocab_size, cắt bớt\n                final_weights = torch.FloatTensor(pretrained_weights_np[:actual_vocab_size, :])\n                print(f\"Cảnh báo: Vocab pretrain ({num_pretrained_words}) lớn hơn vocab của Embedding Layer ({actual_vocab_size}). Đã cắt bớt pretrain.\")\n\n\n            if final_weights.shape[0] == cnn_model.embedding.num_embeddings:\n                cnn_model.embedding.weight.data.copy_(final_weights)\n                cnn_model.embedding.weight.requires_grad = True # Cho phép fine-tuning\n                print(f\"Đã khởi tạo trọng số Embedding từ {vocab_source_for_cnn.upper()}. Embedding trainable.\")\n            else:\n                 print(f\"LỖI kích thước: final_weights.shape[0] ({final_weights.shape[0]}) != cnn_model.embedding.num_embeddings ({cnn_model.embedding.num_embeddings}). Không khởi tạo.\")\n\n        elif original_vectors is not None: # original_vectors có nhưng không khớp điều kiện\n            print(f\"Cảnh báo: Không thể khởi tạo trọng số Embedding cho {model_full_name} từ pretrain. Kích thước không khớp hoặc pretrain rỗng.\")\n            print(f\"  Pretrain shape: {np.array(original_vectors).shape}, Model embedding_dim: {cnn_model.embedding.embedding_dim}\")\n\n\n    optimizer = optim.Adam(cnn_model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses_epoch, train_accs_epoch = [], []\n    test_losses_epoch, test_accs_epoch = [], []\n    \n    current_best_f1_epoch = -1.0\n    best_epoch_model_state = None\n\n    for epoch in range(epochs):\n        train_loss, train_acc = train_cnn_epoch(cnn_model, train_dataloader, optimizer, criterion, device)\n        # Đánh giá trên tập test sau mỗi epoch để có thể chọn model tốt nhất theo epoch\n        test_loss, test_acc, epoch_y_true, epoch_y_pred = evaluate_cnn(cnn_model, test_dataloader, criterion, device)\n        epoch_f1 = f1_score(epoch_y_true, epoch_y_pred, average='macro') # Tính F1 cho epoch này\n        \n        train_losses_epoch.append(train_loss)\n        train_accs_epoch.append(train_acc)\n        test_losses_epoch.append(test_loss)\n        test_accs_epoch.append(test_acc)\n        \n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%, Test F1: {epoch_f1:.4f}\")\n\n        # Lưu state của model tốt nhất trong các epoch của lần chạy CNN này\n        if epoch_f1 > current_best_f1_epoch:\n            current_best_f1_epoch = epoch_f1\n            best_epoch_model_state = cnn_model.state_dict()\n            print(f\"*** New best F1 within this CNN run: {current_best_f1_epoch:.4f} at epoch {epoch+1} ***\")\n\n\n    # Sau khi hoàn thành tất cả các epoch, load lại state tốt nhất và đánh giá lần cuối\n    if best_epoch_model_state:\n        cnn_model.load_state_dict(best_epoch_model_state)\n        print(f\"Đã tải lại model từ epoch có F1 tốt nhất ({current_best_f1_epoch:.4f}) để đánh giá cuối cùng.\")\n    \n    _, _, final_y_true, final_y_pred = evaluate_cnn(cnn_model, test_dataloader, criterion, device)\n    acc_final, f1_final = compute_metrics_updated(final_y_true, final_y_pred, model_full_name + \" (Test Final)\")\n    \n    # Lưu model PyTorch nếu nó tốt nhất toàn cục\n    if f1_final > best_f1_score_global:\n        best_f1_score_global = f1_final\n        model_filename = f\"best_pytorch_model_{model_full_name.replace(' ', '_')}.pth\"\n        best_model_path_global = os.path.join(MODEL_SAVE_DIR, model_filename)\n        best_model_type_global = 'pytorch'\n        \n        # Lưu state_dict và các thông tin cần thiết để tải lại\n        checkpoint = {\n            'model_state_dict': cnn_model.state_dict(), # Hoặc best_epoch_model_state nếu bạn muốn lưu state đó\n            'vocab_source': vocab_source_for_cnn,\n            'preprocessing_type': preprocessing_type,\n            'actual_vocab_size': actual_vocab_size, # Quan trọng\n            'embedding_dim': embedding_dim,\n            'hidden_size': hidden_size,\n            'num_classes': num_classes_cnn,\n            'dropout_rate': dropout_rate,\n            'padding_idx': PAD_ID,\n            'model_architecture_name': model_class.__name__, # Tên lớp model\n            'label_encoder_classes': list(le.classes_) if 'le' in globals() and hasattr(le, 'classes_') else None # Lưu các lớp của LabelEncoder\n        }\n        # Lưu vocab_map nếu là countvec hoặc tfidfvec để có thể tái tạo mapping\n        if vocab_source_for_cnn in ['countvec', 'tfidfvec']:\n            checkpoint['vocab_map'] = vocab_map\n        # Đối với W2V/FT, việc lưu toàn bộ model Gensim có thể tốt hơn nếu muốn dùng lại,\n        # nhưng ở đây ta chỉ lưu vocab_map đã chuẩn hóa cho PyTorch model.\n        elif vocab_source_for_cnn in ['w2v', 'ft']:\n             checkpoint['vocab_map'] = vocab_map # vocab_map đã được chuẩn hóa\n\n        try:\n            torch.save(checkpoint, best_model_path_global)\n            print(f\"Đã lưu model PyTorch tốt nhất mới: {model_full_name} với F1 (Test): {f1_final:.4f} tại {best_model_path_global}\")\n        except Exception as e:\n            print(f\"Lỗi khi lưu model PyTorch {model_full_name}: {e}\")\n            \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1); plt.plot(train_losses_epoch, label='Train Loss'); plt.plot(test_losses_epoch, label='Test Loss'); plt.title(f'Loss - {model_full_name}'); plt.legend()\n    plt.subplot(1, 2, 2); plt.plot(train_accs_epoch, label='Train Acc'); plt.plot(test_accs_epoch, label='Test Acc'); plt.title(f'Accuracy - {model_full_name}'); plt.legend()\n    plt.tight_layout(); plt.show()\n    \n    return acc_final, f1_final, cnn_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.507558Z","iopub.status.idle":"2025-05-20T17:45:12.507839Z","shell.execute_reply.started":"2025-05-20T17:45:12.507684Z","shell.execute_reply":"2025-05-20T17:45:12.507704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_cnn = []\n\ncnn_configurations_setup = [\n    {\"vocab_source\": \"countvec\", \"preprocessing_type\": \"stm\", \"train_tokens\": train_stm_tokens, \"test_tokens\": test_stm_tokens, \"y_train\": y_train_stm, \"y_test\": y_test_stm, \"vectorizer_instance\": count_vec_stm},\n    {\"vocab_source\": \"countvec\", \"preprocessing_type\": \"lem\", \"train_tokens\": train_lem_tokens, \"test_tokens\": test_lem_tokens, \"y_train\": y_train_lem, \"y_test\": y_test_lem, \"vectorizer_instance\": count_vec_lem},\n    \n    {\"vocab_source\": \"tfidfvec\", \"preprocessing_type\": \"stm\", \"train_tokens\": train_stm_tokens, \"test_tokens\": test_stm_tokens, \"y_train\": y_train_stm, \"y_test\": y_test_stm, \"vectorizer_instance\": tfidf_vec_stm},\n    {\"vocab_source\": \"tfidfvec\", \"preprocessing_type\": \"lem\", \"train_tokens\": train_lem_tokens, \"test_tokens\": test_lem_tokens, \"y_train\": y_train_lem, \"y_test\": y_test_lem, \"vectorizer_instance\": tfidf_vec_lem},\n\n    {\"vocab_source\": \"w2v\", \"preprocessing_type\": \"stm\", \"train_tokens\": train_stm_tokens, \"test_tokens\": test_stm_tokens, \"y_train\": y_train_stm, \"y_test\": y_test_stm, \"w2v_model_instance\": w2v_model_stm},\n    {\"vocab_source\": \"w2v\", \"preprocessing_type\": \"lem\", \"train_tokens\": train_lem_tokens, \"test_tokens\": test_lem_tokens, \"y_train\": y_train_lem, \"y_test\": y_test_lem, \"w2v_model_instance\": w2v_model_lem},\n\n    {\"vocab_source\": \"ft\", \"preprocessing_type\": \"stm\", \"train_tokens\": train_stm_tokens, \"test_tokens\": test_stm_tokens, \"y_train\": y_train_stm, \"y_test\": y_test_stm, \"ft_model_instance\": ft_model_stm},\n    {\"vocab_source\": \"ft\", \"preprocessing_type\": \"lem\", \"train_tokens\": train_lem_tokens, \"test_tokens\": test_lem_tokens, \"y_train\": y_train_lem, \"y_test\": y_test_lem, \"ft_model_instance\": ft_model_lem},\n]\n\nfor config in cnn_configurations_setup:\n    acc, f1, _ = run_cnn_experiment(\n        model_class=CNNClassifier,\n        train_tokens_series=config[\"train_tokens\"],\n        test_tokens_series=config[\"test_tokens\"],\n        y_train_series=config[\"y_train\"],\n        y_test_series=config[\"y_test\"],\n        vocab_source_for_cnn=config[\"vocab_source\"],\n        preprocessing_type=config[\"preprocessing_type\"],\n        embedding_dim=EMBEDDING_DIM_CNN,\n        hidden_size=HIDDEN_SIZE_CNN,\n        num_classes_cnn=num_classes,\n        dropout_rate=DROPOUT_RATE_CNN,\n        max_seq_len=MAX_SEQ_LENGTH_CNN,\n        epochs=EPOCHS_CNN,\n        batch_size=BATCH_SIZE_CNN,\n        lr=LEARNING_RATE_CNN,\n        count_vectorizer_instance=config.get(\"vectorizer_instance\") if config[\"vocab_source\"] == \"countvec\" else None,\n        tfidf_vectorizer_instance=config.get(\"vectorizer_instance\") if config[\"vocab_source\"] == \"tfidfvec\" else None,\n        w2v_model_instance=config.get(\"w2v_model_instance\"),\n        ft_model_instance=config.get(\"ft_model_instance\")\n    )\n    model_name_res = f\"CNN_{config['vocab_source'].upper()}_{config['preprocessing_type'].upper()}\"\n    results_cnn.append({\"Model\": model_name_res, \"Accuracy_Test\": acc, \"F1_Macro_Test\": f1})\n\nresults_cnn_df = pd.DataFrame(results_cnn)\nprint(\"\\n--- Bảng tổng kết kết quả CNN ---\")\nprint(results_cnn_df.sort_values(by=\"F1_Macro_Test\", ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.508846Z","iopub.status.idle":"2025-05-20T17:45:12.509117Z","shell.execute_reply.started":"2025-05-20T17:45:12.508979Z","shell.execute_reply":"2025-05-20T17:45:12.508995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not results_ml_df.empty and not results_cnn_df.empty:\n    all_results_df = pd.concat([results_ml_df, results_cnn_df], ignore_index=True)\nelif not results_ml_df.empty:\n    all_results_df = results_ml_df.copy()\nelif not results_cnn_df.empty:\n    all_results_df = results_cnn_df.copy()\nelse:\n    all_results_df = pd.DataFrame()\n\nif not all_results_df.empty:\n    print(\"\\n--- Bảng tổng kết TẤT CẢ KẾT QUẢ ---\")\n    print(all_results_df.sort_values(by=\"F1_Macro_Test\", ascending=False))\n    \n    print(f\"\\n--- Model tốt nhất toàn cục ---\")\n    print(f\"Loại model: {best_model_type_global}\")\n    print(f\"F1 Score (Macro) trên tập Test: {best_f1_score_global:.4f}\")\n    print(f\"Đường dẫn lưu model: {best_model_path_global}\")\n    if best_model_type_global == 'pytorch':\n        print(\"Để tải lại model PyTorch, bạn cần định nghĩa lại kiến trúc model và sử dụng torch.load() trên checkpoint.\")\n    elif best_model_type_global == 'sklearn':\n        print(\"Để tải lại model scikit-learn, sử dụng: loaded_model = joblib.load(best_model_path_global)\")\n\nelse:\n    print(\"Không có kết quả nào để hiển thị.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:45:12.509721Z","iopub.status.idle":"2025-05-20T17:45:12.509921Z","shell.execute_reply.started":"2025-05-20T17:45:12.509828Z","shell.execute_reply":"2025-05-20T17:45:12.509837Z"}},"outputs":[],"execution_count":null}]}